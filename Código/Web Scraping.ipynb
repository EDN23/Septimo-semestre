{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b379c2-4a73-48d7-9a9f-48f2b5131b3d",
   "metadata": {},
   "source": [
    "### Ejercicio de Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173a88af-f249-41d6-984b-843d31eefd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerias que usaremos\n",
    "\n",
    "import pandas as pd #Nod ayudará a hacer un DF con la información obtenida\n",
    "from selenium import webdriver #Permite automatizar navegadores web para interactuar con sitios web (como abrir páginas, hacer clic en botones o llenar formularios).\n",
    "from bs4 import BeautifulSoup as bs #para analizar y manipular documentos HTML o XML de manera sencilla. Es ideal para extraer datos específicos de un sitio web.\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager # Se encarga de descargar automáticamente el controlador (driver) correcto para el navegador que estás usando (en este caso, Chrome). Evita que tengas que descargar manualmente el chromedriver compatible con tu navegador.\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configuración\n",
    "\n",
    "# Configuramos el driver de chrome\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "titulos = []\n",
    "precios = []\n",
    "stocks = []\n",
    "\n",
    "url = \"http://books.toscrape.com\"\n",
    "\n",
    "while True:\n",
    "    titulos_sucios = []\n",
    "    precios_sucios = []\n",
    "    stocks_sucio = []\n",
    "\n",
    "    # urlaux = urljoin(url,f\"/catalogue/page-{page}.html\")\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Extracción de datos\n",
    "    contenido = driver.page_source\n",
    "\n",
    "    soup = bs(contenido)\n",
    "\n",
    "    articles = soup.find_all(\"article\",attrs={\"class\":\"product_pod\"})\n",
    "\n",
    "    for libro in articles:\n",
    "        titulo = libro.find(\"h3\")\n",
    "        titulos_sucios.append(titulo.text)\n",
    "\n",
    "        precio = libro.find(\"p\",attrs={\"class\":\"price_color\"})\n",
    "        precios_sucios.append(precio.text)\n",
    "\n",
    "        stock = libro.find(\"p\",attrs={\"class\":\"instock availability\"})\n",
    "        stocks_sucio.append(stock.text)\n",
    "\n",
    "    # Limpiando los datos, Se eliminan caracteres no deseados (como saltos de línea).\n",
    "    for libro in titulos_sucios:\n",
    "        titulos.append(libro.replace(\"\\n\",\"\").strip())\n",
    "\n",
    "    for libro in precios_sucios:\n",
    "        precios.append(libro.replace(\"\\n\",\"\").strip())\n",
    "\n",
    "    for libro in stocks_sucio:\n",
    "        stocks.append(libro.replace(\"\\n\",\"\").strip())\n",
    "    \n",
    "    siguientepag = soup.select_one(\"li.next>a\")\n",
    "    if siguientepag:\n",
    "        siguienteurl = siguientepag.get(\"href\")\n",
    "        url = urljoin(url, siguienteurl)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "# Almacenamiento datos\n",
    "#Los datos se almacenan en un DataFrame de pandas y luego se exportan a un archivo .csv.\n",
    "df = pd.DataFrame({\"titulo\":titulos,\"precio\":precios,\"stock\":stocks})\n",
    "df.head()\n",
    "\n",
    "# Exportando los datos a un .csv\n",
    "df.to_csv(\"libros.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
